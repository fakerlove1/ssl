{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mixmatch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gan3sh500/mixmatch-pytorch/blob/master/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmES8DZG7pFc",
        "colab_type": "text"
      },
      "source": [
        "This notebook tries to implement the MixMatch technique from the [paper](https://arxiv.org/pdf/1905.02249.pdf) MixMatch: A Holistic Approach to Semi-Supervised Learning and recreate their results on CIFAR10 with WideResnet28. \n",
        "\n",
        "It depends on Pytorch, Numpy and imgaug. The WideResnet28 model code is taken from [meliketoy](https://github.com/meliketoy/wide-resnet.pytorch/blob/master/networks/wide_resnet.py)'s github repository. Hopefully I can train this on Colab with a Tesla T4. :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3ROGJfXigq3",
        "colab_type": "code",
        "outputId": "58e098b9-1e6b-434e-f695-2dad12be9ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 23 14:30:40 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   62C    P8    17W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkPKM_FeXCUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import imgaug.augmenters as iaa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_V6d_r-8QUi",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the basic imports out of the way lets get to it. \n",
        "First we shall define the function to get augmented version of a given batch of images. The below function returns the function to do that. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKrQ2XsBXLlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_augmenter():\n",
        "    seq = iaa.Sequential([\n",
        "        iaa.Crop(px=(0, 16)),\n",
        "        iaa.Fliplr(0.5),\n",
        "        iaa.GaussianBlur(sigma=(0, 3.0))\n",
        "    ])\n",
        "    def augment(images):\n",
        "        return seq.augment(images.transpose(0, 2, 3, 1)).transpose(0, 2, 3, 1)\n",
        "    return augment"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se8HRC8z8byR",
        "colab_type": "text"
      },
      "source": [
        "Next we define the sharpening function to sharpen the prediction from the averaged prediction of all the unlabeled augmented images. It does the same thing as applying a temperature within the softmax function but to the probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_DDDq0qYP5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sharpen(x, T):\n",
        "    temp = x**(1/T)\n",
        "    return temp / temp.sum(axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhvvJUKN80lU",
        "colab_type": "text"
      },
      "source": [
        "A simple implementation of the [paper](https://arxiv.org/pdf/1710.09412.pdf) mixup: Beyond Empirical Risk Minimization used in this paper as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q21aM3biiVgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixup(x1, x2, y1, y2, alpha):\n",
        "    beta = np.random.beta(alpha, -alpha)\n",
        "    x = beta * x1 + (1 - beta) * x2\n",
        "    y = beta * y1 + (1 - beta) * y2\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU0JHbCh90o5",
        "colab_type": "text"
      },
      "source": [
        "This covers Algorithm 1 from the paper. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2Yi1WWiZNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixmatch(x, y, u, model, augment_fn, T=0.5, K=2, alpha=0.75):\n",
        "    xb = augment_fn(x)\n",
        "    ub = [augment_fn(u) for _ in range(K)]\n",
        "    qb = sharpen(sum(map(lambda i: model(i), ub)) / K, T)\n",
        "    Ux = np.concatenate(ub, axis=0)\n",
        "    Uy = np.concatenate([qb for _ in range(K)], axis=0)\n",
        "    indices = np.random.shuffle(np.arange(len(xb) + len(Ux)))\n",
        "    Wx = np.concatenate([Ux, xb], axis=0)[indices]\n",
        "    Wy = np.concatenate([qb, y], axis=0)[indices]\n",
        "    X, p = mixup(xb, Wx[:len(xb)], y, Wy[:len(xb)], alpha)\n",
        "    U, q = mixup(Ux, Wx[len(xb):], Uy, Wy[len(xb):], alpha)\n",
        "    return X, U, p, q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmSvUmiP94zT",
        "colab_type": "text"
      },
      "source": [
        "The combined loss for training from the paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5ylws-0kziT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MixMatchLoss(torch.nn.Module):\n",
        "    def __init__(self, lambda_u=100):\n",
        "        self.lambda_u = lambda_u\n",
        "        self.xent = torch.nn.CrossEntropyLoss()\n",
        "        self.mse = torch.nn.MSELoss()\n",
        "        super(MixMatchLoss, self).__init__()\n",
        "    \n",
        "    def forward(self, X, U, p, q, model):\n",
        "        X_ = np.concatenate([X, U], axis=1)\n",
        "        preds = model(X_)\n",
        "        return self.xent(preds[:len(p)], p) + \\\n",
        "                                    self.lambda_u * self.mse(preds[len(p):], q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCqJtpJ--Cik",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the MixMatch stuff done, we have a few things to do. Namely, define the WideResnet28 model, write the data and training code and write testing code. \n",
        "Let's start with the model. The below is just a copy paste mostly from the wide-resnet.pytorch repo by meliketoy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIkBy3T15P7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return torch.nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                           bias=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fud8CmEtCaSN",
        "colab_type": "text"
      },
      "source": [
        "Will need the below init function later before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZBBH5EYCZhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
        "        torch.nn.init.constant(m.bias, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.constant(m.weight, 1)\n",
        "        torch.nn.init.constant(m.bias, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_gOfar1CeUx",
        "colab_type": "text"
      },
      "source": [
        "The basic block for the WideResnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ068XQR6LZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WideBasic(torch.nn.Module):\n",
        "    def __init__(self, in_planes, planes, dropout_rate, stride=1):\n",
        "        super(WideBasic, self).__init__()\n",
        "        self.bn1 = torch.nn.BatchNorm2d(in_planes)\n",
        "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
        "        self.conv1 = torch.nn.Conv2d(in_planes, planes, kernel_size=3,\n",
        "                                     padding=1, bias=True)\n",
        "        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                                     padding=1, bias=True)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "        self.shortcut = torch.nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = torch.nn.Sequential(\n",
        "                torch.nn.Conv2d(in_planes, planes, kernel_size=1,\n",
        "                                stride=stride, bias=True)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.dropout(self.conv1(torch.nn.functional.relu(self.bn1(x))))\n",
        "        out = self.conv2(torch.nn.functional.relu(self.bn2(out)))\n",
        "        return out + self.shortcut(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdew7GNoChmh",
        "colab_type": "text"
      },
      "source": [
        "Aaand the full model with default params set for CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvE9l4W27jTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WideResNet(torch.nn.Module):\n",
        "    def __init__(self, depth=28, widen_factor=10,\n",
        "                 dropout_rate=0.3, num_classes=10):\n",
        "        super(WideResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        n = (depth - 4) // 6\n",
        "        k = widen_factor\n",
        "        nStages = [16, 16*k, 32*k, 64*k]\n",
        "        self.conv1 = conv3x3(3, nStages[0])\n",
        "        self.layer1 = self.wide_layer(WideBasic, nStages[1], n, dropout_rate,\n",
        "                                      stride=1)\n",
        "        self.layer2 = self.wide_layer(WideBasic, nStages[2], n, dropout_rate,\n",
        "                                      stride=2)\n",
        "        self.layer3 = self.wide_layer(WideBasic, nStages[3], n, dropout_rate,\n",
        "                                      stride=2)\n",
        "        self.b1 = torch.nn.BatchNorm2d(nStages[3], momentum=0.9)\n",
        "        self.linear = torch.nn.Linear(nStages[3], num_classes)\n",
        "    \n",
        "    def wide_layer(self, block, planes, num_blocks, dropout_rate, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, dropout_rate, stride))\n",
        "            self.in_planes = planes\n",
        "        return torch.nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.layer3(self.layer2(self.layer1(out)))\n",
        "        out = torch.nn.functional.relu(self.bn1(out))\n",
        "        out = torch.nn.functional.avg_pool2d(out, 8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        return self.linear(out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnLX7FkIEz1L",
        "colab_type": "text"
      },
      "source": [
        "Now that we have the model let's write train and test loaders so that we can pass the model and the data to the MixMatchLoss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjCTPM8wB-dR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic_generator(x, y=None, batch_size=32, shuffle=True):\n",
        "    i = 0\n",
        "    all_indices = np.random.shuffle(np.arange(len(x))) if shuffle else \\\n",
        "                                                               np.arange(len(x))\n",
        "    while(True):\n",
        "        indices = all_indices[i:i+batch_size]\n",
        "        if y is not None:\n",
        "            yield x[indices], y[indices]\n",
        "        yield x[indices]\n",
        "        i = (i + batch_size) % len(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQb89EOHfUH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mixmatch_wrapper(x, y, u, model, batch_size=32):\n",
        "    augment_fn = get_augmenter()\n",
        "    train_generator = basic_generator(x, y, batch_size)\n",
        "    unlabeled_generator = basic_generator(u, batch_size=batch_size)\n",
        "    while(True):\n",
        "        xi, yi = next(train_generator)\n",
        "        ui = next(unlabeled_generator)\n",
        "        yield mixmatch(xi, yi, ui, model, augment_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLafEafCJthx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_torch(*args, device='cuda'):\n",
        "    convert_fn = lambda x: torch.from_numpy(x).to(device)\n",
        "    return list(map(convert_fn, args))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSTVdcWriKTq",
        "colab_type": "text"
      },
      "source": [
        "That about covers all the code we need for train and test loaders. Now we can start the training and evaluation. Let's see if all of this works or is just a mess. Going to add basically this same training code from meliketoy's repo but with the MixMatchLoss. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRJOZ9FLL40g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_gen, test_iters):\n",
        "    acc = []\n",
        "    for i, (x, y) in enumerate(test_gen):\n",
        "        x = to_torch(x)\n",
        "        pred = model(x).to('cpu').argmax(axis=1)\n",
        "        acc.append(np.mean(pred == y.argmax(axis=1)))\n",
        "        if i == test_iters:\n",
        "            break\n",
        "    print('Accuracy was : {}'.format(np.mean(acc)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNac4RKMMvln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def report(loss_history):\n",
        "    print('Average loss in last epoch was : {}'.format(np.mean(loss_history)))\n",
        "    return []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_IeC5TXNHIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save(model, iter, train_iters):\n",
        "    torch.save(model.state_dict(), 'model_{}.pth'.format(train_iters // iters))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAMKAUNtiZwV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(model, train_gen, test_gen, epochs, train_iters, test_iters, device):\n",
        "    optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = MixMatchLoss()\n",
        "    loss_history = []\n",
        "    for i, (x, u, p, q) in enumerate(train_gen):\n",
        "        if i % train_iters == 0:\n",
        "            loss_history = report(loss_history)\n",
        "            test(model, test_gen, test_iters)\n",
        "            save(model, i, train_iters)\n",
        "            if i // train_iters == epochs:\n",
        "                return\n",
        "        else:\n",
        "            optim.zero_grad()\n",
        "            x, u, p, q = to_torch(x, u, p, q, device=device)\n",
        "            loss = loss_fn(x, u, p, q, model)\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            loss_history.append(loss.to('cpu'))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}